 <!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Occlusion-Robust Relative Pose Estimation for Multi-Robot Systems via Geometric-Aware Diffusion Matching">
  <meta name="keywords" content="GADM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Occlusion-Robust Relative Pose Estimation for Multi-Robot Systems via Geometric-Aware Diffusion Matching</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.ico"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>
<body>

</section>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Occlusion-Robust Relative Pose Estimation for Multi-Robot Systems via Geometric-Aware Diffusion Matching</h1>
          </div>
          <div class="is-size-5 publication-authors mt-0">
            <span class="author-block">
              Author Names omitted for Anonymous Review.</span>
            <!-- <span class="author-block">
              Peng Gao<sup>2</sup>,</span>
            <span class="author-block">
              Williard Joshua Jose<sup>1</sup>,</span>
            <span class="author-block">
              Christopher Reardon<sup>3</sup>,</span>
            <span class="author-block">
              Maggie Wigness<sup>4</sup>,</span>
            <span class="author-block">
              John Rogers<sup>4</sup>, and</span>
            <span class="author-block">
              Hao Zhang<sup>1</sup></span> -->
          </div>

          <div class="is-size-6 publication-authors">
            <!-- <span class="author-block">
              <sup>1</sup> Human-Centered Robotics Lab @ UMass Amherst, </span>
            <span class="author-block">
              <sup>2</sup> North Carolina State University, </span>
            <span class="author-block">
              <sup>3</sup> University of Denver, </span>
            <span class="author-block">
              <sup>4</sup> U.S. Army DEVCOM Army Research Laboratory</span> -->
          </div>

          <div class="column has-text-centered">
            <span class="link-block">
              <!-- <a class="external-link" href="https://hcr.cs.umass.edu">
                  <img src="./static/images/hcrlab-logo.png" alt="HCR Lab logo" 
                        width="200" />
              </a> -->
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/pdf/main_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="24" height="24" stroke-width="1.5" stroke="currentColor" class="size-6">
                      <path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z" />
                    </svg>
                                        
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!--/ PDF Link. -->
              
              <!-- Supp Link. -->
              <span class="link-block">
                <a href="./static/pdf/supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="24" height="24" stroke-width="1.5" stroke="currentColor" class="size-6">
                      <path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m3.75 9v6m3-3H9m1.5-12H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z" />
                    </svg>
                      
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>
              <!--/ Supp Link. -->
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24" fill="currentColor">
                      <path d="M12 0C5.373 0 0 5.373 0 12c0 5.303 3.438 9.8 8.207 11.387.6.11.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.744.083-.729.083-.729 1.205.085 1.839 1.237 1.839 1.237 1.07 1.834 2.809 1.305 3.495.998.108-.775.419-1.305.762-1.605-2.665-.305-5.466-1.334-5.466-5.93 0-1.31.468-2.382 1.236-3.222-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.3 1.23a11.499 11.499 0 0 1 3.004-.404c1.019.005 2.043.138 3.004.404 2.29-1.552 3.297-1.23 3.297-1.23.654 1.652.243 2.873.12 3.176.77.84 1.234 1.912 1.234 3.222 0 4.608-2.805 5.623-5.475 5.921.43.372.814 1.102.814 2.222v3.293c0 .32.193.694.8.576C20.565 21.798 24 17.303 24 12 24 5.373 18.627 0 12 0z"/>
                    </svg>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!--/ Code Link. -->

              <!-- Poster Link. -->
              <!-- <span class="link-block">
                <a href="./static/pdf/AFOR_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" width="24" height="24" stroke-width="1.5" stroke="currentColor" class="size-6">
                      <path stroke-linecap="round" stroke-linejoin="round" d="M12 7.5h1.5m-1.5 3h1.5m-7.5 3h7.5m-7.5 3h7.5m3-9h3.375c.621 0 1.125.504 1.125 1.125V18a2.25 2.25 0 0 1-2.25 2.25M16.5 7.5V18a2.25 2.25 0 0 0 2.25 2.25M16.5 7.5V4.875c0-.621-.504-1.125-1.125-1.125H4.125C3.504 3.75 3 4.254 3 4.875V18a2.25 2.25 0 0 0 2.25 2.25h13.5M6 7.5h3v3H6v-3Z" />
                    </svg>                    
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->
              <!--/ Poster Link. -->
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract-->
    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Relative pose estimation is crucial for coordinated multi-robot navigation. 
            However, robots in close proximity often face intra-team occlusions, 
            where teammates partially block each other's field of view, 
            while dynamic environments further introduce environmental occlusions. 
            Classical relative pose estimation methods degrade under occlusion and texture scarcity, 
            whereas learning-based methods often lack explicit geometric consistency, which limits their accuracy during real deployments.
            To address multi-robot relative pose estimation in complex 3D environments, 
            we introduce \textit{Geometric-Aware Diffusion Matching} (GADM), 
            which enables a team of robots to estimate relative 6-DoF poses using only RGB-D sensors, even under occlusions. 
            GADM uses a diffusion model to progressively exploit global and higher-order structural constraints encoded by a graph network, 
            guiding smoother optimization and faster convergence to robust correspondence distributions under noise and occlusions.
            By integrating geometric consistency, GADM explicitly addresses occlusions by producing geometrically consistent matches suitable for real-time deployment on physical robots. 
            The resulting correspondences are then used with geometry-based solvers to estimate 6-DoF relative poses, providing robustness even under partial view overlap and limited keypoint visibility. 
            We conducted experiments using both robotics simulations and physical robot teams, and our results show that GADM achieves robust 6-DoF pose estimation performance in occluded scenarios.
          </p>
        </div>
      </div>
    </div>

    <!-- Motivation and Approach Section -->
    <div class="columns is-centered mt-6">
      <div class="column">
        <div class="content">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h4 class="title is-4">Motivation</h4>
              <img src="./static/images/motivation.png"
                   class="interpolation-image"
                   alt="Motivation image"
                   style="width: 100%;" />
            </div>
            <div class="column">
              <h4 class="title is-4 has-text-centered">Approach</h4>
              <img src="./static/images/approach.png"
                   class="interpolation-image"
                   alt="Approach image"
                   style="width: 100%;" />
              <p>
                Overview GADM.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- /Motivation and Approach Section -->
  </div>
</section>

  </div>
</section>

<!-- Demo Videos. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Real World Section -->
    <div class="columns is-centered mt-0 mb-0">
      <div class="column">
        <div class="content">
          <h3 class="title is-4 has-text-centered">Results on Physical Robots in Real Outdoor Scenarios</h3>
        </div>
      </div>
    </div>
    <div class="columns is-centered mb-6">
      <div class="column">
        <p class="mb-2">
          We validate our system through a downstream formation control task in GPS-denied outdoor environments, explicitly demonstrating the real-time applicability of GADM. Robots operate on uneven terrain with frequent mutual occlusions, dynamic pedestrians, moving obstacles, and varying illumination. The trained GADM model can run in real time at $\sim$10 Hz (trained model runs at $\sim$10 Hz standalone, but the full perceptionâ€“control loop runs at $\sim$5 Hz) on an Intel i7-13620H CPU with an NVIDIA RTX~4050 GPU mounted on the B1 legged robot. To implement a leader-follower wedge formation, we set the B1 robot to serve as the leader, while the tracked Bunker and wheeled Jackal robots act as followers. Each follower robot (Jackal and Bunker) broadcasts its observations to the leader (B1), which estimates the relative poses using GADM. With synchronized RGB-D observations streamed to the remote node, B1 performs pairwise relative pose estimation via GADM and broadcasts the resulting estimates back to the follower robots. This configuration preserves a map-free design: no pre-built map, GPS, or infrastructure is required beyond standard time synchronization and sensor-to-base extrinsics calibration. By avoiding global mapping and relying solely on inter-robot observations, the system remains decentralized and agnostic to platform morphology, as long as extrinsics are known.
The demo videos highlight robustness under dynamic pedestrians and varying light conditions.
        </p>
        <div class="columns is-variable is-1">
          <div class="column">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="./static/videos/outdoor_bridge.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="./static/videos/outdoor_bridge_turn.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="./static/videos/outdoor_lawn.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Real World Section -->


</body>
</html>
